{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae54a66",
   "metadata": {},
   "source": [
    "#### **Hypothetical Prompt Embeddings (HyPE) Question -> Question Retrieval**\n",
    "\n",
    "Before adding chunks to DB, we use LLM to create questions corresponding to each chunk and then create embeddings of these questions.\n",
    "\n",
    "Now, while creating vectorDB we'll embedding for each question - attach chunk corresponding to it and store it in a vectorDB.\n",
    "\n",
    "Now at time of retrieval we'll send the query and it tries to look or find queries similar to it.\n",
    "\n",
    "HyPE provides a scalable and efficient alternative to traditional RAG systems, overcoming query-document style mismatch.\n",
    "\n",
    "---\n",
    "\n",
    "#### **LLM used :** \n",
    "\n",
    "from langchain_ollama -> ChatOllama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "363e575f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, so I don't have emotions or feelings in the way that humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-10T16:46:57.415794Z', 'done': True, 'done_reason': 'stop', 'total_duration': 21909666167, 'load_duration': 4147801417, 'prompt_eval_count': 30, 'prompt_eval_duration': 12583611042, 'eval_count': 49, 'eval_duration': 3712220415, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--9fcb0e1f-2536-43d9-b7d5-3cf21146716a-0', usage_metadata={'input_tokens': 30, 'output_tokens': 49, 'total_tokens': 79})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='llama3.2',\n",
    "    temeprature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hello How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579807d6",
   "metadata": {},
   "source": [
    "#### **Embedding Model**\n",
    "\n",
    "We'll use HuggingFace Embeddings from langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30a348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embeddings are : 384\n",
      "Embeddings sample : [-0.013380538672208786, 0.003255972173064947, 0.10806030035018921, 0.08322358131408691, 0.02040085941553116, -0.049066152423620224, 0.0722508355975151, 0.002980925841256976, -0.08823534101247787, 0.016058299690485, -0.03367079421877861, -4.332493062975118e-06, -0.02510129101574421, 0.0007887802203185856, 0.060331884771585464, -0.0415474958717823, 0.07702311128377914, -0.14256997406482697, -0.13958506286144257, 0.06023767963051796, 0.003192346775904298, 0.018982844427227974, 0.02300790697336197, 0.06056844815611839, -0.07911035418510437, -0.05399537831544876, -0.0008475205395370722, 0.03202424943447113, -0.029674910008907318, -0.04484577104449272, -0.10411098599433899, 0.06399180740118027, -0.05713418126106262, -0.02695028856396675, -0.028776653110980988, 0.00333896791562438, -0.0355900302529335, -0.13525626063346863, 0.009469274431467056, 0.0003555373114068061, 0.009924577549099922, -0.0014938903041183949, -0.009747199714183807, -0.0021706046536564827, 0.06437141448259354, -0.04134561866521835, -0.015959464013576508, 0.07168345153331757, 0.09831950813531876, 0.010891384445130825, -0.10564935207366943, -0.06154218316078186, -0.04495823755860329, 0.03288338705897331, 0.06448811292648315, 0.026930589228868484, -0.022059716284275055, 0.008090948686003685, 0.0885234996676445, -0.040078986436128616, -0.011959749273955822, 0.025354033336043358, -0.10569483041763306, 0.0038036201149225235, 0.059116121381521225, -0.016421761363744736, -0.06289204210042953, -0.03538224473595619, -0.03384290635585785, -0.05755750089883804, -0.01964549534022808, -0.07316037267446518, 0.03075730986893177, -0.017475150525569916, 0.029496576637029648, -0.08609264343976974, 0.052657350897789, -0.04254796728491783, 0.03953508660197258, 0.04585482180118561, 0.06450486183166504, -0.06745240837335587, 0.002348054898902774, 0.03787226229906082, -0.0785246193408966, -0.11359238624572754, 0.052211660891771317, 0.07972294092178345, -0.018256613984704018, 0.010808249935507774, -0.08069567382335663, 0.03911193832755089, 0.03796018287539482, -0.027921967208385468, 0.034581851214170456, -0.024894041940569878, 0.10931583493947983, 0.0032324434723705053, -0.05208413302898407, 0.11619603633880615]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')\n",
    "\n",
    "sample_text = \"Hey How are you?\"\n",
    "\n",
    "sample_embeddings = embedding_model.embed_query(sample_text)\n",
    "print(f\"Length of embeddings are : {len(sample_embeddings)}\")\n",
    "print(f\"Embeddings sample : {sample_embeddings[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba7bcd",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##### **Loading the PDF data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ceafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Docs : 33\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "\n",
    "file_path = \"../data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path) \n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"Number of Docs : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8eeab7",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##### **Making Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabc56ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks : 170\n",
      "Sample chunks : Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. \n",
      "Historical Context\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Number of Chunks : {len(chunks)}\")\n",
    "print(f\"Sample chunks : {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dad269f",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##### **Using LLM to create questions corresponding to each chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b93f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we need to create a Data Class\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated, List \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "class QuestionGen(BaseModel):\n",
    "    \"\"\" \n",
    "    Generating list of questions that can be answered from context or information present in Chunks.\n",
    "    \"\"\"\n",
    "    questions: Annotated[List[str], Field(description=\"List of questions that can be answered using content present in Chunk.\")]\n",
    "\n",
    "## configuring LLM \n",
    "llm_for_question_gen = llm.with_structured_output(QuestionGen)\n",
    "\n",
    "## template to generate question for given Chunks \n",
    "template_question_gen = \"\"\" \n",
    "\"Analyze the input text and generate essential 2 questions that, when answered, \\\n",
    "        capture the main points of the text. Each question should be one line, \\\n",
    "        without numbering or prefixes.\\n\\n \\\n",
    "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_template_question_gen = PromptTemplate(\n",
    "    template=template_question_gen,\n",
    "    input_variables=['chunk_text']\n",
    ")\n",
    "\n",
    "question_gen_chain = prompt_template_question_gen | llm_for_question_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b9ad0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Chunk : Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. \n",
      "Historical Context\n",
      "-----------------------------------------------------------------------------------------\n",
      "Sample questions : ['What is the term for significant long-term changes in the global climate?', 'What are the main human activities contributing to climate change?']\n"
     ]
    }
   ],
   "source": [
    "# testing whether it generates question from chunks\n",
    "sample_chunk = chunks[0].page_content \n",
    "print(f\"Sample Chunk : {sample_chunk}\")\n",
    "print(\"-\"*89)\n",
    "questions_generated = question_gen_chain.invoke({'chunk_text' : sample_chunk})\n",
    "print(f\"Sample questions : {questions_generated.questions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b108d0",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "##### **Making a function that takes Chunks -> create questions -> create embeddings and store it to vectorDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab1a1b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
