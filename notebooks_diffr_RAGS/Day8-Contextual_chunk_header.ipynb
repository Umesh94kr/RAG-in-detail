{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784d3caf",
   "metadata": {},
   "source": [
    "#### **Contextual Chunk Headers**\n",
    "\n",
    "In this approach we give each chunk a Title or a brief header information (that can represent true content of that chunk)\n",
    "\n",
    "This feature leads to enhancement in retrieval quality.\n",
    "\n",
    "##### **Flow**\n",
    "- Create chunks from document\n",
    "- Create title corresponding to each chunk using LLM\n",
    "- Add those titles with chunks and create embeddings and store them in vectorstore\n",
    "- Create a retriever | make a RAG flow | test your RAG\n",
    "- You can do a performance test for retriever with CCH and without CCH\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc3d64c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have! How can I help you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-13T15:31:29.535412Z', 'done': True, 'done_reason': 'stop', 'total_duration': 23526228084, 'load_duration': 5070066834, 'prompt_eval_count': 29, 'prompt_eval_duration': 12635922250, 'eval_count': 47, 'eval_duration': 3979463504, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--de1d15ae-e5e7-4b8a-beb3-e8d282b00a09-0', usage_metadata={'input_tokens': 29, 'output_tokens': 47, 'total_tokens': 76})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM used for inference is Llama3.2 from langchain_ollama \n",
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='llama3.2',\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm.invoke(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cefdb8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sample embeddings : 384\n",
      "Sample embedding : [-0.013380538672208786, 0.003255972173064947, 0.10806030035018921, 0.08322358131408691, 0.02040085941553116, -0.049066152423620224, 0.0722508355975151, 0.002980925841256976, -0.08823534101247787, 0.016058299690485, -0.03367079421877861, -4.332493062975118e-06, -0.02510129101574421, 0.0007887802203185856, 0.060331884771585464, -0.0415474958717823, 0.07702311128377914, -0.14256997406482697, -0.13958506286144257, 0.06023767963051796, 0.003192346775904298, 0.018982844427227974, 0.02300790697336197, 0.06056844815611839, -0.07911035418510437, -0.05399537831544876, -0.0008475205395370722, 0.03202424943447113, -0.029674910008907318, -0.04484577104449272, -0.10411098599433899, 0.06399180740118027, -0.05713418126106262, -0.02695028856396675, -0.028776653110980988, 0.00333896791562438, -0.0355900302529335, -0.13525626063346863, 0.009469274431467056, 0.0003555373114068061, 0.009924577549099922, -0.0014938903041183949, -0.009747199714183807, -0.0021706046536564827, 0.06437141448259354, -0.04134561866521835, -0.015959464013576508, 0.07168345153331757, 0.09831950813531876, 0.010891384445130825, -0.10564935207366943, -0.06154218316078186, -0.04495823755860329, 0.03288338705897331, 0.06448811292648315, 0.026930589228868484, -0.022059716284275055, 0.008090948686003685, 0.0885234996676445, -0.040078986436128616, -0.011959749273955822, 0.025354033336043358, -0.10569483041763306, 0.0038036201149225235, 0.059116121381521225, -0.016421761363744736, -0.06289204210042953, -0.03538224473595619, -0.03384290635585785, -0.05755750089883804, -0.01964549534022808, -0.07316037267446518, 0.03075730986893177, -0.017475150525569916, 0.029496576637029648, -0.08609264343976974, 0.052657350897789, -0.04254796728491783, 0.03953508660197258, 0.04585482180118561, 0.06450486183166504, -0.06745240837335587, 0.002348054898902774, 0.03787226229906082, -0.0785246193408966, -0.11359238624572754, 0.052211660891771317, 0.07972294092178345, -0.018256613984704018, 0.010808249935507774, -0.08069567382335663, 0.03911193832755089, 0.03796018287539482, -0.027921967208385468, 0.034581851214170456, -0.024894041940569878, 0.10931583493947983, 0.0032324434723705053, -0.05208413302898407, 0.11619603633880615]\n"
     ]
    }
   ],
   "source": [
    "# embedding model which we'll use is from Sentence Transformers provided by langchain_huggingface\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model='all-MiniLM-L6-v2')\n",
    "\n",
    "sample_embeddings = embedding_model.embed_query(\"Hey How are you?\")\n",
    "print(f\"Length of sample embeddings : {len(sample_embeddings)}\")\n",
    "print(f\"Sample embedding : {sample_embeddings[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e260f17",
   "metadata": {},
   "source": [
    "#### **Loading the Data and Making Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85af4ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs : 33\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/Understanding_Climate_Change.pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "\n",
    "docs = loader.load()\n",
    "print(f\"Number of docs : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a746a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Chunks are : 215\n",
      "Sample chunk : Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human\n"
     ]
    }
   ],
   "source": [
    "# Using text splitter to make chunks \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Number of Chunks are : {len(chunks)}\")\n",
    "print(f\"Sample chunk : {chunks[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dac723",
   "metadata": {},
   "source": [
    "#### **Making a chain that can create title corresponding to each Chunk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bbcd41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Output Data Structure\n",
    "class TitleOChunk(BaseModel):\n",
    "    \"\"\" \n",
    "    This will return a string that represents suitable title for a Chunk.\n",
    "    \"\"\"\n",
    "    title: Annotated[str, Field(description=\"Suitable title to the Chunk.\")]\n",
    "\n",
    "# configuring LLM with data structure \n",
    "llm_title_gen = llm.with_structured_output(TitleOChunk)\n",
    "\n",
    "# template\n",
    "title_gen_template = \"\"\" \n",
    "    You are given with a chunk : {chunk}.\n",
    "\n",
    "    On the basis of given chunk, generate a title suitable for it.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    template=title_gen_template,\n",
    "    input_variables=['chunk']\n",
    ")\n",
    "\n",
    "# creating a chain \n",
    "title_gen_chain = prompt_template | llm_title_gen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7709cf69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title suggested : Fossil Fuels and Climate Change\n",
      "-----------------------------------------------------------------------------------------\n",
      "Chunk : page_content='activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal' metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2024-07-13T20:17:34+03:00', 'author': 'Nir', 'moddate': '2024-07-13T20:17:34+03:00', 'source': '../data/Understanding_Climate_Change.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "## lets test this out \n",
    "response = title_gen_chain.invoke({'chunk' : chunks[6]})\n",
    "print(f\"Title suggested : {response.title}\")\n",
    "print(\"-\"*89)\n",
    "print(f\"Chunk : {chunks[6]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18b26a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 215/215 [13:07<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# we'll make a function that will add these Title to the starting of the Chunk\n",
    "# and creates new Chunks \n",
    "from tqdm import tqdm\n",
    "\n",
    "def add_title_to_chunks(chunks):\n",
    "    for chunk in tqdm(chunks):\n",
    "        title_generated = title_gen_chain.invoke({'chunk' : chunk.page_content})\n",
    "        chunk.page_content = title_generated.title + \"\\n\\n\" + chunk.page_content\n",
    "add_title_to_chunks(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea9767c",
   "metadata": {},
   "source": [
    "##### **After this**\n",
    "- make a vector_store\n",
    "- create retriever \n",
    "- build RAG class \n",
    "- use it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068db6bd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
