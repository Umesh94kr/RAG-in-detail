{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c1a69c",
   "metadata": {},
   "source": [
    "#### **Contextual Compression**\n",
    "\n",
    "- When we retrieve chunks from Retriever, they most of the text within those chunks is irrelevant. \n",
    "\n",
    "- So on top of that retriever we'll apply another contextual compression retriever that will compress the contextual chunks for us.\n",
    "\n",
    "#### **Advantages**\n",
    "\n",
    "- **Improved Relevancy** -> Chunks would be more relevant to the query asked.\n",
    "- **Increased efficiency** -> As the size of chunks is decreased, token consumption would go down and also time to process this information.\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"../contextual_compression.png\" width=\"400\" height=\"700\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "##### **LLM used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168e5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umesh/Desktop/RAG-in-detail/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"I'm an AI designed to assist and communicate with users like you. I'm a large language model, which means I can understand and respond to natural language inputs.\\n\\nRight now, I'm waiting for your next question or prompt. You can ask me anything, from general knowledge questions to creative writing prompts, and I'll do my best to provide helpful and accurate responses.\\n\\nIf you need help with something specific, feel free to ask, and I'll get started!\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-12-19T16:41:45.946513Z', 'done': True, 'done_reason': 'stop', 'total_duration': 26135700666, 'load_duration': 3636857958, 'prompt_eval_count': 30, 'prompt_eval_duration': 12807944417, 'eval_count': 94, 'eval_duration': 6692792916, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--dfb3093d-4223-4548-9f56-b7eb1b2776de-0' usage_metadata={'input_tokens': 30, 'output_tokens': 94, 'total_tokens': 124}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='llama3.2',\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(llm.invoke(\"What are you doing?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1d2f9",
   "metadata": {},
   "source": [
    "##### **Embedding Model used**\n",
    "\n",
    "From Huggingface sentence transformers we used HuggingFaceEmbeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d60c1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text embedding : 24\n",
      "Time taken to convert text to embedding : 4.93 sec\n",
      "[-0.0383385606110096, 0.1234646886587143, -0.02864295430481434, 0.05365273356437683, 0.0088453618809...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "import time \n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "\n",
    "start = time.time()\n",
    "query_result = embedding_model.embed_query(text)\n",
    "total_time = time.time() - start\n",
    "# show only the first 100 characters of the stringified vector\n",
    "print(f\"Length of text embedding : {len(text)}\")\n",
    "print(f\"Time taken to convert text to embedding : {total_time :.2f} sec\")\n",
    "print(str(query_result)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e0b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this embedding model to create embeddings of text present\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate \n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated, List \n",
    "\n",
    "# making a data validation class \n",
    "class CompressedChunks(BaseModel):\n",
    "    \"\"\"\n",
    "    This will create chunks which are from the original chunks but are compressed tto small length and contains the context.\n",
    "    \"\"\"\n",
    "\n",
    "    compressed_chunk: Annotated[str, Field(description=\"This will be compressed chunk which contains most of the context of original chunk but shoreter in length.\")]\n",
    "\n",
    "\n",
    "# configure the llm with structured output \n",
    "compress_chunk_llm = llm.with_structured_output(CompressedChunks)\n",
    "\n",
    "# writing system Prompt for this task of compressing \n",
    "compress_chunks = \"\"\" \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
