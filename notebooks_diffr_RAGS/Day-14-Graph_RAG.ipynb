{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a0f07e3",
   "metadata": {},
   "source": [
    "#### **GraphRAG: Graph-Enhanced Retrieval-Augmented Generation**\n",
    "\n",
    "- GraphRAG is an advanced question-answering system that combines the power of graph-based knowledge representation with retrieval-augmented generation. \n",
    "- It processes input documents to create a rich knowledge graph, which is then used to enhance the retrieval and generation of answers to user queries. \n",
    "- The system leverages natural language processing, machine learning, and graph theory to provide more accurate and contextually relevant responses.\n",
    "\n",
    "#### **Motivation**\n",
    "\n",
    "Traditional retrieval-augmented generation systems often struggle with maintaining context over long documents and making connections between related pieces of information. GraphRAG addresses these limitations by:\n",
    "\n",
    "- Representing knowledge as an interconnected graph, allowing for better preservation of relationships between concepts.\n",
    "- Enabling more intelligent traversal of information during the query process.\n",
    "Providing a visual representation of how information is connected and accessed during the answering process.\n",
    "\n",
    "#### **Key Components**\n",
    "\n",
    "- **DocumentProcessor**: Handles the initial processing of input documents, creating text chunks and embeddings.\n",
    "\n",
    "- **KnowledgeGraph**: Constructs a graph representation of the processed documents, where nodes represent text chunks and edges represent relationships between them.\n",
    "\n",
    "- **QueryEngine**: Manages the process of answering user queries by leveraging the knowledge graph and vector store.\n",
    "\n",
    "- **Visualizer**: Creates a visual representation of the graph and the traversal path taken to answer a query.\n",
    "\n",
    "#### **Method Details**\n",
    "\n",
    "4) #### **Vizualization** \n",
    "    - The knowledge graph is visualized with nodes representing text chunks and edges representing relationships.\n",
    "    - Edge colors indicate the strength of relationships (weights).\n",
    "    - The traversal path taken to answer a query is highlighted with curved, dashed arrows.\n",
    "    - Start and end nodes of the traversal are distinctly colored for easy identification.\n",
    "\n",
    "#### **Conclusion**\n",
    "\n",
    "- GraphRAG represents a significant advancement in retrieval-augmented generation systems. By incorporating a graph-based knowledge representation and intelligent traversal mechanisms, it offers improved context awareness, more accurate retrieval, and enhanced explainability. \n",
    "\n",
    "- The system's ability to visualize its decision-making process provides valuable insights into its operation, making it a powerful tool for both end-users and developers.\n",
    "\n",
    "- As natural language processing and graph-based AI continue to evolve, systems like GraphRAG pave the way for more sophisticated and capable question-answering technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a8f12dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "# from langchain.callbacks import get_openai_callback\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Tuple, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import heapq\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from spacy.cli import download\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "# from helper_functions import *\n",
    "# from evaluation.evalute_rag import *\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48564",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **LLM used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ab658aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='llama3.2',\n",
    "    verbose=True,\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hey How are you?\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028fc47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Embedding Model** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6763145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of embeddings : 384\n",
      "Sample embeddings : [-0.013380538672208786, 0.003255972173064947, 0.10806030035018921, 0.08322358131408691, 0.02040085941553116, -0.049066152423620224, 0.0722508355975151, 0.002980925841256976, -0.08823534101247787, 0.016058299690485, -0.03367079421877861, -4.332493062975118e-06, -0.02510129101574421, 0.0007887802203185856, 0.060331884771585464, -0.0415474958717823, 0.07702311128377914, -0.14256997406482697, -0.13958506286144257, 0.06023767963051796, 0.003192346775904298, 0.018982844427227974, 0.02300790697336197, 0.06056844815611839, -0.07911035418510437, -0.05399537831544876, -0.0008475205395370722, 0.03202424943447113, -0.029674910008907318, -0.04484577104449272, -0.10411098599433899, 0.06399180740118027, -0.05713418126106262, -0.02695028856396675, -0.028776653110980988, 0.00333896791562438, -0.0355900302529335, -0.13525626063346863, 0.009469274431467056, 0.0003555373114068061, 0.009924577549099922, -0.0014938903041183949, -0.009747199714183807, -0.0021706046536564827, 0.06437141448259354, -0.04134561866521835, -0.015959464013576508, 0.07168345153331757, 0.09831950813531876, 0.010891384445130825, -0.10564935207366943, -0.06154218316078186, -0.04495823755860329, 0.03288338705897331, 0.06448811292648315, 0.026930589228868484, -0.022059716284275055, 0.008090948686003685, 0.0885234996676445, -0.040078986436128616, -0.011959749273955822, 0.025354033336043358, -0.10569483041763306, 0.0038036201149225235, 0.059116121381521225, -0.016421761363744736, -0.06289204210042953, -0.03538224473595619, -0.03384290635585785, -0.05755750089883804, -0.01964549534022808, -0.07316037267446518, 0.03075730986893177, -0.017475150525569916, 0.029496576637029648, -0.08609264343976974, 0.052657350897789, -0.04254796728491783, 0.03953508660197258, 0.04585482180118561, 0.06450486183166504, -0.06745240837335587, 0.002348054898902774, 0.03787226229906082, -0.0785246193408966, -0.11359238624572754, 0.052211660891771317, 0.07972294092178345, -0.018256613984704018, 0.010808249935507774, -0.08069567382335663, 0.03911193832755089, 0.03796018287539482, -0.027921967208385468, 0.034581851214170456, -0.024894041940569878, 0.10931583493947983, 0.0032324434723705053, -0.05208413302898407, 0.11619603633880615]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = embedding_model.embed_query(\"Hey How are you?\")\n",
    "print(f\"Length of embeddings : {len(embeddings)}\")\n",
    "print(f\"Sample embeddings : {embeddings[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd79cea7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Loading the Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69070fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Docs : 33\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "file_path = \"../data/Understanding_Climate_Change.pdf\"\n",
    "\n",
    "loader = PyPDFLoader(file_path)\n",
    "docs = loader.load()\n",
    "print(f\"Number of Docs : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcababe4",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "#### **Document Processor**\n",
    "- Input documents are split into manageable chunks.\n",
    "- Each chunk is embedded using a language model.\n",
    "- A vector store is created from these embeddings for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17ac3e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss \n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Define the DocumentProcessor class\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, llm, embedding_model):\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embedding_model \n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        splits = self.text_splitter.split_documents(documents)\n",
    "        vector_store = FAISS.from_documents(splits, self.embedding_model)\n",
    "        return splits, vector_store\n",
    "    \n",
    "    def create_embeddings_batch(self, texts, batch_size=32):\n",
    "        embeddings = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_embeddings = self.embeddings.embed_documents(batch)\n",
    "            embeddings.extend(batch_embeddings)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def compute_similarity_matrix(self, embeddings):\n",
    "        return cosine_similarity(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d87baf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Knowledge Graph Construction:** \n",
    "\n",
    "- Graph nodes are created for each text chunk.\n",
    "- Concepts are extracted from each chunk using a combination of NLP techniques and language models.\n",
    "- Extracted concepts are lemmatized to improve matching.\n",
    "- Edges are added between nodes based on semantic similarity and shared concepts.\n",
    "- Edge weights are calculated to represent the strength of relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b521cf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Concepts class\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Annotated\n",
    "import networkx as nx \n",
    "\n",
    "# Data Validation Model \n",
    "class Concepts(BaseModel):\n",
    "    concepts_list: Annotated[List[str], Field(description=\"List of concepts\")]\n",
    "    \n",
    "# Define the KnowledgeGraph class\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph() # networkx Graph \n",
    "        self.lemmatizer = WordNetLemmatizer() # An instance of WordNetLemmatizer\n",
    "        self.concept_cache = {} # A dictionary to cache extracted concepts (content maps to concept)\n",
    "        self.nlp = self._load_spacy_model()  \n",
    "        self.edges_threshold = 0.8 \n",
    "    \n",
    "    def build_graph(self, splits, llm, embedding_model):\n",
    "        self._add_nodes(splits)\n",
    "        embeddings = self._create_embeddings(splits, embedding_model)\n",
    "        self._extract_concepts(splits, llm)\n",
    "        self._add_edges(embeddings)\n",
    "\n",
    "    def _add_nodes(self, splits):\n",
    "        \"Adding nodes to the Graph.\"\n",
    "        for i, split in enumerate(splits):\n",
    "            self.graph.add_node(i, content=split.page_content)\n",
    "\n",
    "    def _create_embeddings(self, splits, embedding_model):\n",
    "        \"Get embeddings for each text chunk we have.\"\n",
    "        texts = [split.page_content for split in splits]\n",
    "        return embedding_model.embed_documents(texts)\n",
    "\n",
    "    def _compute_similarities(self, embeddings):\n",
    "        \"Cosine similarity of embeddings\"\n",
    "        return cosine_similarity(embeddings)\n",
    "\n",
    "    def _load_spacy_model(self):\n",
    "        \"Load the Spacy Model.\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            print(\"Downloading spaCy model...\")\n",
    "            download(\"en_core_web_sm\")\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def _extract_concepts_and_entities(self, content, llm):\n",
    "        if content in self.concept_cache:\n",
    "            return self.concept_cache[content]\n",
    "        # Extract named entities using spaCy\n",
    "        doc = self.nlp(content)\n",
    "        named_entities = [ent.text for ent in doc.ents if ent.label_ in [\"PERSON\", \"ORG\", \"GPE\", \"WORK_OF_ART\"]]\n",
    "        # Extract general concepts using LLM\n",
    "        concept_extraction_prompt = PromptTemplate(\n",
    "            input_variables=[\"text\"],\n",
    "            template=\"Extract key concepts (excluding named entities) from the following text:\\n\\n{text}\\n\\nKey concepts:\"\n",
    "        )\n",
    "        concept_chain = concept_extraction_prompt | llm.with_structured_output(Concepts)\n",
    "        general_concepts = concept_chain.invoke({\"text\": content}).concepts_list\n",
    "        \n",
    "        # Combine named entities and general concepts\n",
    "        all_concepts = list(set(named_entities + general_concepts))\n",
    "        self.concept_cache[content] = all_concepts\n",
    "        return all_concepts\n",
    "\n",
    "    def _extract_concepts(self, splits, llm):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            future_to_node = {executor.submit(self._extract_concepts_and_entities, split.page_content, llm): i \n",
    "                              for i, split in enumerate(splits)}\n",
    "            \n",
    "            for future in tqdm(as_completed(future_to_node), total=len(splits), desc=\"Extracting concepts and entities\"):\n",
    "                node = future_to_node[future]\n",
    "                concepts = future.result()\n",
    "                self.graph.nodes[node]['concepts'] = concepts\n",
    "\n",
    "    def _add_edges(self, embeddings):\n",
    "        similarity_matrix = self._compute_similarities(embeddings)\n",
    "        num_nodes = len(self.graph.nodes)\n",
    "        \n",
    "        for node1 in tqdm(range(num_nodes), desc=\"Adding edges\"):\n",
    "            for node2 in range(node1 + 1, num_nodes):\n",
    "                similarity_score = similarity_matrix[node1][node2]\n",
    "                if similarity_score > self.edges_threshold:\n",
    "                    shared_concepts = set(self.graph.nodes[node1]['concepts']) & set(self.graph.nodes[node2]['concepts'])\n",
    "                    edge_weight = self._calculate_edge_weight(node1, node2, similarity_score, shared_concepts)\n",
    "                    self.graph.add_edge(node1, node2, weight=edge_weight, \n",
    "                                        similarity=similarity_score,\n",
    "                                        shared_concepts=list(shared_concepts))\n",
    "\n",
    "    def _calculate_edge_weight(self, node1, node2, similarity_score, shared_concepts, alpha=0.7, beta=0.3):\n",
    "        max_possible_shared = min(len(self.graph.nodes[node1]['concepts']), len(self.graph.nodes[node2]['concepts']))\n",
    "        normalized_shared_concepts = len(shared_concepts) / max_possible_shared if max_possible_shared > 0 else 0\n",
    "        return alpha * similarity_score + beta * normalized_shared_concepts\n",
    "\n",
    "    def _lemmatize_concept(self, concept):\n",
    "        return ' '.join([self.lemmatizer.lemmatize(word) for word in concept.lower().split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1950ed6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Query Engine**\n",
    "\n",
    "- The user query is embedded and used to retrieve relevant documents from the vector store.\n",
    "- A priority queue is initialized with the nodes corresponding to the most relevant documents.\n",
    "- The system employs a Dijkstra-like algorithm to traverse the knowledge graph:\n",
    "    - Nodes are explored in order of their priority (strength of connection to the query).\n",
    "    - For each explored node:\n",
    "        - Its content is added to the context.\n",
    "        - The system checks if the current context provides a complete answer.\n",
    "        - If the answer is incomplete:\n",
    "            - The node's concepts are processed and added to a set of visited concepts.\n",
    "            - Neighboring nodes are explored, with their priorities updated based on edge weights.\n",
    "            - Nodes are added to the priority queue if a stronger connection is found.\n",
    "- This process continues until a complete answer is found or the priority queue is exhausted.\n",
    "- If no complete answer is found after traversing the graph, the system generates a final answer using the accumulated context and a large language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ebf1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pdb\n",
    "\n",
    "# Define the AnswerCheck class for Structured output from LLM \n",
    "class AnswerCheck(BaseModel):\n",
    "    is_complete: bool = Field(description=\"Whether the current context provides a complete answer to the query\")\n",
    "    answer: str = Field(description=\"The current answer based on the context, if any\")\n",
    "\n",
    "# Define the QueryEngine class\n",
    "class QueryEngine:\n",
    "    def __init__(self, vector_store, knowledge_graph, llm):\n",
    "        # it would take vector_store, knowledge_graph and llm \n",
    "        self.vector_store = vector_store\n",
    "        self.knowledge_graph = knowledge_graph \n",
    "        self.llm = llm\n",
    "        self.max_context_length = 4000 # defining the context size\n",
    "        self.answer_check_chain = self._create_answer_check_chain()\n",
    "\n",
    "    # this is the chain for checking whether context is enough to provide the answer to questions or not\n",
    "    def _create_answer_check_chain(self):\n",
    "        answer_check_prompt = PromptTemplate(\n",
    "            input_variables=[\"query\", \"context\"],\n",
    "            template=\"Given the query: '{query}'\\n\\nAnd the current context:\\n{context}\\n\\nDoes this context provide a complete answer to the query? If yes, provide the answer. If no, state that the answer is incomplete.\\n\\nIs complete answer (Yes/No):\\nAnswer (if complete):\"\n",
    "        )\n",
    "        return answer_check_prompt | self.llm.with_structured_output(AnswerCheck)\n",
    "\n",
    "    # invoking check-answer chain\n",
    "    def _check_answer(self, query: str, context: str) -> Tuple[bool, str]:\n",
    "        response = self.answer_check_chain.invoke({\"query\": query, \"context\": context})\n",
    "        return response.is_complete, response.answer\n",
    "\n",
    "    def _expand_context(self, query: str, relevant_docs):\n",
    "        expanded_context = \"\"\n",
    "        traversal_path = []\n",
    "        visited_nodes = set()\n",
    "        visited_concepts = set()\n",
    "        filtered_content = {}\n",
    "        final_answer = \"\"\n",
    "\n",
    "        priority_queue = []\n",
    "        distances = {}\n",
    "\n",
    "        print(\"\\nTraversing the knowledge graph:\")\n",
    "\n",
    "        # --- Seed the queue from vector retrieval ---\n",
    "        for doc in relevant_docs:\n",
    "            node_doc, score = self.vector_store.similarity_search_with_score(\n",
    "                doc.page_content, k=1\n",
    "            )[0]\n",
    "\n",
    "            start_node = next(\n",
    "                n for n in self.knowledge_graph.graph.nodes\n",
    "                if self.knowledge_graph.graph.nodes[n][\"content\"] == node_doc.page_content\n",
    "            )\n",
    "\n",
    "            priority = 1 / score\n",
    "            distances[start_node] = priority\n",
    "            heapq.heappush(priority_queue, (priority, start_node))\n",
    "\n",
    "        step = 0\n",
    "\n",
    "        # --- Best-first graph traversal ---\n",
    "        while priority_queue:\n",
    "            current_priority, current_node = heapq.heappop(priority_queue)\n",
    "\n",
    "            # Skip stale queue entries\n",
    "            if current_priority > distances.get(current_node, float(\"inf\")):\n",
    "                continue\n",
    "\n",
    "            # Skip already expanded nodes\n",
    "            if current_node in visited_nodes:\n",
    "                continue\n",
    "\n",
    "            # --- Expand node ---\n",
    "            visited_nodes.add(current_node)\n",
    "            traversal_path.append(current_node)\n",
    "            step += 1\n",
    "\n",
    "            node_data = self.knowledge_graph.graph.nodes[current_node]\n",
    "            node_content = node_data[\"content\"]\n",
    "            node_concepts = node_data[\"concepts\"]\n",
    "\n",
    "            filtered_content[current_node] = node_content\n",
    "            expanded_context += \"\\n\" + node_content if expanded_context else node_content\n",
    "\n",
    "            print(f\"\\nStep {step} - Node {current_node}\")\n",
    "            print(f\"Content: {node_content[:100]}...\")\n",
    "            print(f\"Concepts: {', '.join(node_concepts)}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            # --- Early stopping check ---\n",
    "            is_complete, answer = self._check_answer(query, expanded_context)\n",
    "            if is_complete:\n",
    "                final_answer = answer\n",
    "                break\n",
    "\n",
    "            # --- Concept gating ---\n",
    "            lemmatized = {\n",
    "                self.knowledge_graph._lemmatize_concept(c)\n",
    "                for c in node_concepts\n",
    "            }\n",
    "\n",
    "            if lemmatized.issubset(visited_concepts):\n",
    "                continue\n",
    "\n",
    "            visited_concepts.update(lemmatized)\n",
    "\n",
    "            # --- Queue neighbors ---\n",
    "            for neighbor in self.knowledge_graph.graph.neighbors(current_node):\n",
    "                edge_weight = self.knowledge_graph.graph[current_node][neighbor][\"weight\"]\n",
    "                new_distance = current_priority + (1 / edge_weight)\n",
    "\n",
    "                if new_distance < distances.get(neighbor, float(\"inf\")):\n",
    "                    distances[neighbor] = new_distance\n",
    "                    heapq.heappush(priority_queue, (new_distance, neighbor))\n",
    "\n",
    "        # --- Fallback answer ---\n",
    "        if not final_answer:\n",
    "            response_prompt = PromptTemplate(\n",
    "                input_variables=[\"query\", \"context\"],\n",
    "                template=(\n",
    "                    \"Based on the following context, answer the query.\\n\\n\"\n",
    "                    \"Context:\\n{context}\\n\\nQuery:\\n{query}\\n\\nAnswer:\"\n",
    "                )\n",
    "            )\n",
    "            response_chain = response_prompt | self.llm\n",
    "            final_answer = response_chain.invoke({\n",
    "                \"query\": query,\n",
    "                \"context\": expanded_context\n",
    "            })\n",
    "\n",
    "        return expanded_context, traversal_path, filtered_content, final_answer\n",
    "\n",
    "\n",
    "    def query(self, query: str) -> Tuple[str, List[int], Dict[int, str]]:\n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        print(f\"Getting relevant documents from vectorstore \")\n",
    "        relevant_docs = self._retrieve_relevant_documents(query)\n",
    "        # this is some complex function\n",
    "        expanded_context, traversal_path, filtered_content, final_answer = self._expand_context(query, relevant_docs)\n",
    "            \n",
    "        if not final_answer:\n",
    "            print(\"\\nGenerating final answer...\")\n",
    "            response_prompt = PromptTemplate(\n",
    "                input_variables=[\"query\", \"context\"],\n",
    "                template=\"Based on the following context, please answer the query\\n\\nContext: {context}\\n\\nQuery: {query}\\n\\nAnswer:\"\n",
    "            )\n",
    "                \n",
    "            response_chain = response_prompt | self.llm\n",
    "            input_data = {\"query\": query, \"context\": expanded_context}\n",
    "            response = response_chain.invoke(input_data)\n",
    "            final_answer = response\n",
    "        else:\n",
    "            print(\"\\nComplete answer found during traversal.\")\n",
    "            \n",
    "        print(f\"\\nFinal Answer: {final_answer}\")\n",
    "        \n",
    "        return final_answer, traversal_path, filtered_content\n",
    "\n",
    "    def _retrieve_relevant_documents(self, query: str):\n",
    "        print(\"\\nRetrieving relevant documents...\")\n",
    "        retriever = self.vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "        # compressor = LLMChainExtractor.from_llm(self.llm)\n",
    "        # compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "        return retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "458d08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb \n",
    "\n",
    "class GraphRAG:\n",
    "    def __init__(self, llm, embedding_model):\n",
    "        self.llm = llm\n",
    "        self.embedding_model = embedding_model\n",
    "        self.document_processor = DocumentProcessor(llm, embedding_model)\n",
    "        self.knowledge_graph = KnowledgeGraph()\n",
    "        self.query_engine = None\n",
    "        # self.visualizer = Visualizer()\n",
    "\n",
    "    def process_documents(self, documents):\n",
    "        splits, vector_store = self.document_processor.process_documents(documents)\n",
    "        self.knowledge_graph.build_graph(splits, self.llm, self.embedding_model)\n",
    "        self.query_engine = QueryEngine(vector_store, self.knowledge_graph, self.llm)\n",
    "\n",
    "    def query(self, query: str):\n",
    "        response, traversal_path, filtered_content = self.query_engine.query(query)\n",
    "        # if traversal_path:\n",
    "        #     self.visualizer.visualize_traversal(self.knowledge_graph.graph, traversal_path)\n",
    "        # else:\n",
    "        #     print(\"No traversal path to visualize.\")\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "59a399f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting concepts and entities: 100%|██████████| 215/215 [15:05<00:00,  4.21s/it]\n",
      "Adding edges: 100%|██████████| 215/215 [00:00<00:00, 32905.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "graph_rag = GraphRAG(llm, embedding_model)\n",
    "\n",
    "# lets process the documents \n",
    "graph_rag.process_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b7fc95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing query: What is Climate Change?\n",
      "Getting relevant documents from vectorstore \n",
      "\n",
      "Retrieving relevant documents...\n",
      "\n",
      "Traversing the knowledge graph:\n",
      "\n",
      "Step 1 - Node 131\n",
      "Content: Chapter 14: Climate Change and the Economy \n",
      "Economic Transformation...\n",
      "Concepts: Economic Transformation, Economy, the Economy \n",
      "Economic Transformation, Climate Change\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 2 - Node 101\n",
      "Content: impacts of climate change. This includes studying the links between climate and health, \n",
      "developing ...\n",
      "Concepts: climate education, curriculum development, Advocacy \n",
      "Climate Education \n",
      "Curriculum Development, evidence-based policies and interventions, Education, health data systems, links between climate and health, new technologies and treatments\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 3 - Node 4\n",
      "Content: provide a historical record that scientists use to understand past climate conditions and \n",
      "predict f...\n",
      "Concepts: emission, greenhouse gases, trends, climate change\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 4 - Node 0\n",
      "Content: Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to si...\n",
      "Concepts: global, temperature, wind patterns, long-term, weather patterns, precipitation\n",
      "--------------------------------------------------\n",
      "\n",
      "Step 5 - Node 5\n",
      "Content: Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in ...\n",
      "Concepts: greenhouse gases, CH4, greenhouse effect, atmosphere, climate change, heat trap\n",
      "--------------------------------------------------\n",
      "\n",
      "Complete answer found during traversal.\n",
      "\n",
      "Final Answer: content='Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet\\'s overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-12-31T03:29:04.600811Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5797202583, 'load_duration': 161558292, 'prompt_eval_count': 329, 'prompt_eval_duration': 2688650500, 'eval_count': 43, 'eval_duration': 2943080291, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'} id='lc_run--019b7273-9fb0-7a11-80b3-b7961444e1bd-0' usage_metadata={'input_tokens': 329, 'output_tokens': 43, 'total_tokens': 372}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Climate change refers to significant, long-term changes in the global climate. The term \"global climate\" encompasses the planet\\'s overall weather patterns, including temperature, precipitation, and wind patterns, over an extended period.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-31T03:29:04.600811Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5797202583, 'load_duration': 161558292, 'prompt_eval_count': 329, 'prompt_eval_duration': 2688650500, 'eval_count': 43, 'eval_duration': 2943080291, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--019b7273-9fb0-7a11-80b3-b7961444e1bd-0', usage_metadata={'input_tokens': 329, 'output_tokens': 43, 'total_tokens': 372})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_rag.query(\"What is Climate Change?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9060e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
