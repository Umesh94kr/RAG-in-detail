{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21e68f7d",
   "metadata": {},
   "source": [
    "#### **Query Enhancement**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554926a0",
   "metadata": {},
   "source": [
    "Other than (optimizing chunking we can also look at QueryEnhancement techniques that just explain or add some more details to our query to make it better - Better the query Better the retrieval)\n",
    "\n",
    "We have 3 different approaches for this : \n",
    "\n",
    "- Query rewriting : In which query is reformulated using LLM to add more details\n",
    "\n",
    "- Query expansion : Generate broader queries containing same intent and semantics as your current query\n",
    "\n",
    "- Sub-query Decomposition : This technique can be use to break a complex query into sub-queries.\n",
    "\n",
    "**Main Goal** - Getting comprehensive retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b502d",
   "metadata": {},
   "source": [
    "#### **LLM used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c051d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/umesh/Desktop/RAG-in-detail/myenv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, so I don't have emotions or feelings like humans do. However, I'm functioning properly and ready to help with any questions or tasks you may have! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-05T09:59:44.650567Z', 'done': True, 'done_reason': 'stop', 'total_duration': 20997181625, 'load_duration': 3630311333, 'prompt_eval_count': 30, 'prompt_eval_duration': 12778679959, 'eval_count': 46, 'eval_duration': 3182832337, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--03c27077-a03e-4033-b4a7-c9fe5e37bdf5-0', usage_metadata={'input_tokens': 30, 'output_tokens': 46, 'total_tokens': 76})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama \n",
    "\n",
    "llm = ChatOllama(\n",
    "    model='llama3.2',\n",
    "    temperature=0,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hey How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a36ce9",
   "metadata": {},
   "source": [
    "#### **Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b074a0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of embeddings : 384\n",
      "[-0.0383385606110096, 0.1234646886587143, -0.02864295430481434, 0.05365273356437683, 0.0088453618809...\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings \n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embedding_model.embed_query(text)\n",
    "\n",
    "# show only the first 100 characters of the stringified vector\n",
    "print(f\"Dimension of embeddings : {len(query_result)}\")\n",
    "print(str(query_result)[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ab548",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d1faf",
   "metadata": {},
   "source": [
    "#### **1. Query Rewriting**\n",
    "\n",
    "In this we tell LLM to make the query detailed and concise (reformulate it), so that retrieval can be improved "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eeca7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated \n",
    "\n",
    "# making a Output Data Schema for LLM \n",
    "class QueryRewriting(BaseModel):\n",
    "    \"\"\" \n",
    "    Return the re-written query from LLM\n",
    "    \"\"\"\n",
    "    rewritten_query: Annotated[str, Field(description=\"Rewritten query\")]\n",
    "\n",
    "# configuring LLM for this re-written query output \n",
    "llm_query_rewriting = llm.with_structured_output(QueryRewriting)\n",
    "\n",
    "query_rewriting_prompt = PromptTemplate.from_template(\"You are an AI assistant tasked with reformulating the queries\" \\\n",
    "\"to improve RAG retrieval. Given the original query, rewrite it to be more specific, detailed, and likely to retrieve relevant information. \" \\\n",
    "\"Original query : {query}\" \\\n",
    "\"Rewritten query : \")\n",
    "\n",
    "# making a chain\n",
    "rewriting_query_chain = query_rewriting_prompt | llm_query_rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5bffe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Re-written query : What are the most significant environmental impacts of '\n",
      " 'climate change, including changes in temperature, sea level rise, and '\n",
      " 'extreme weather events, as well as their effects on biodiversity, '\n",
      " 'ecosystems, and human health?')\n"
     ]
    }
   ],
   "source": [
    "## lets try to re-write a query\n",
    "from pprint import pprint\n",
    "\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "response = rewriting_query_chain.invoke({'query' : {query}})\n",
    "pprint(f\"Re-written query : {response.rewritten_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ada9925",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **2. Query Expansion**\n",
    "\n",
    "In this we try to generate more queries which contains the same semantic and intent as the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "438c6158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate \n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated \n",
    "\n",
    "class QueryExpansion(BaseModel):\n",
    "    \"\"\"\n",
    "    This will create new query, which contains same semantic and intent as original query.\n",
    "    \"\"\"\n",
    "    expanded_query: Annotated[str, Field(description=\"The new query after applying query expansion.\")]\n",
    "\n",
    "## configuring LLM with the structured output \n",
    "llm_query_exp = llm.with_structured_output(QueryExpansion)\n",
    "\n",
    "## writing system prompt for Query expansion \n",
    "query_exp_template = \"\"\" \n",
    "You are an AI assistant tasked with generating broader, more general queries to improve context retrieval in a RAG system.\n",
    "Given the original query, generate an expanded query that is more general and can help retrieve relevant background information.\n",
    "\n",
    "Original query: {original_query}\n",
    "\n",
    "expanded_query : \n",
    "\"\"\"\n",
    "\n",
    "query_expansion_prompt = PromptTemplate(\n",
    "    template=query_exp_template, \n",
    "    input_variables=['original_query']\n",
    ")\n",
    "\n",
    "# query expansion chain \n",
    "query_expansion_chain = query_expansion_prompt | llm_query_exp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c25d640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original_query : What are the impacts of climate change on the environment?\n",
      "Expanded query : expanded_query='What are the effects of global warming on ecosystems, biodiversity, and natural resources?'\n",
      "Time taken : 21.39 sec\n"
     ]
    }
   ],
   "source": [
    "# lets try this using a dummy query\n",
    "import time\n",
    "\n",
    "query = \"What are the impacts of climate change on the environment?\"\n",
    "print(f\"Original_query : {query}\")\n",
    "start = time.time()\n",
    "expanded_query = query_expansion_chain.invoke({'original_query' : query})\n",
    "end = time.time()\n",
    "print(f\"Expanded query : {expanded_query}\")\n",
    "print(f\"Time taken : {end-start :.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94e1c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **3. Subquery Decomposition** \n",
    "\n",
    "If query is really complex then we can use an LLM to break this complex query into subqueries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6830af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import Annotated, List\n",
    "\n",
    "# we need to make a data output class that LLM can use to give structured output \n",
    "class SubqueryDecomposition(BaseModel):\n",
    "    \"\"\"\n",
    "    This will return a List of simple queries that are break-down from one complex query. \n",
    "    \"\"\"\n",
    "    subqueries: Annotated[List[str], Field(description=\"List of subqueries breaked down from a complex query.\")]\n",
    "\n",
    "# configuring LLM with data class\n",
    "llm_for_subquery_decomp = llm.with_structured_output(SubqueryDecomposition)\n",
    "\n",
    "subquery_decomposition_template = \"\"\"You are an AI assistant tasked with breaking down complex queries into simpler sub-queries for a RAG system.\n",
    "Given the original query, decompose it into 2-4 simpler sub-queries that, when answered together, would provide a comprehensive response to the original query.\n",
    "\n",
    "Original query: {original_query}\n",
    "subqueries: (You need to generate)\n",
    "\n",
    "example: What are the impacts of climate change on the environment?\n",
    "\n",
    "subqueries:[\"What are the impacts of climate change on biodiversity?\", \"How does climate change affect the oceans?\", \"What are the effects of climate change on agriculture?\", \"What are the impacts of climate change on human health?\"]\"\"\"\n",
    "\n",
    "subquery_decomposition_prompt = PromptTemplate(\n",
    "    template=subquery_decomposition_template,\n",
    "    input_variables=['original_query']\n",
    ")\n",
    "\n",
    "# llm chain for subquery decomposition \n",
    "chain_for_subquery_decomp = subquery_decomposition_prompt | llm_for_subquery_decomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "295f1b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-queries : ['What is the current state of global greenhouse gas emissions?', 'How do changes in temperature and precipitation patterns affect ecosystems?', 'What are the most significant economic costs associated with climate change?', 'What role do climate change mitigation strategies play in reducing environmental impacts?']\n",
      "Time taken : 24.67 sec\n"
     ]
    }
   ],
   "source": [
    "# Lets try this subquery decomposition\n",
    "original_query = \"What are the impacts of climate change on the environment?\"\n",
    "start = time.time()\n",
    "response = chain_for_subquery_decomp.invoke({'original_query' : original_query})\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Sub-queries : {response.subqueries}\")\n",
    "print(f\"Time taken : {end-start :.2f} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
